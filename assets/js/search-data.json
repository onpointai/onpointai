{
  
    
        "post0": {
            "title": "Using Deep Learning to detect Pneumonia in X-ray images",
            "content": "About . This blog is towards the Course Project for the [Pytorch Zero to GANS] free online course(https://jovian.ml/forum/c/pytorch-zero-to-gans/18) run by JOVIAN.ML. . The course competition was based on analysing protein cells with muti-label classification. . Therefore, to extend my understanding of dealing with medical imaging I decided to use the X-Ray image database in Kaggle. . Seeing as I ran out of GPU hours on Kaggle because of the competition (restricted to 30hrs/week at the time of writing June 2020) I opted to use Google Colab. . This blog is in the form of a Jupyter notebook and inspired by link. . The blog talks about getting the dataset in Google Colab, explore the dataset, develop the training model, metrics and then does some preliminary training to get a model which is then used to make a few predictions. I will then talk about some of the lessons learned. . Warning! The purpose of this blog is to outline the steps taken in a typical Machine Learning project and should be treated as such. . The full notebook on Google Colab is here. It is worth taking a peek just to see the monokai UI as snippet of which is whon below: . Import libraries . import os import torch import pandas as pd import time import copy import PIL import numpy as np from torch.utils.data import Dataset, random_split, DataLoader from PIL import Image import torchvision from torchvision import datasets import torchvision.models as models import matplotlib.pyplot as plt from tqdm.notebook import tqdm import torchvision.transforms as T from sklearn.metrics import f1_score import torch.nn.functional as F import torch.nn as nn from torch.optim import lr_scheduler from collections import OrderedDict from torchvision.utils import make_grid from torch.autograd import Variable import seaborn as sns import csv %matplotlib inline . Colab setup and getting data from Kaggle . I used Google Colab with GPU processing for this project because I had exhausted my Kaggle hours (30hrs/wk) working on the competition :( The challenge here was signing into Colab, setting up the working directoty and then linking to Kaggle and copying the data over. The size of the dataset was about 1.3Gb which wasn’t too much of a bother as Google gives each Gmail account 15Gb for free! . Tip: I used the monokai settings in Colab which gave excellent font contrast and colours for editing. . . from google.colab import drive drive.mount(&#39;/content/drive&#39;, force_remount=True) . The default directory that is linked to the Google’s gdrive ( the one connected to the gmail address) is /content/drive/My Drive/ . I create a project directory jovian-xray and use this as the new root directory. . root_dir = &#39;/content/drive/My Drive/jovian-xray&#39; os.chdir(root_dir) !pwd os.mkdir(&#39;kaggle&#39;) . Install Kaggle in your current Colab session. Log into Kaggle, point to the dataset and copy the API key. This downloads a kaggle.json file. Upload this kaggle.json to Colab. . !pip install -q kaggle from google.colab import files . Select the kaggle.json file. This will be uploaded to your current working directory which is the root_dir as specified above. Create a ./kaggle directory in the home directory Copy the kaggle.json from the current directory to this new directory. Change permissions so that it can be executed by user and group. . upload = files.upload() !mkdir ~/.kaggle !ls !cp kaggle.json ~/.kaggle/ !chmod 600 ~/.kaggle/kaggle.json . proj_dir = os.path.join(root_dir, &#39;kaggle&#39;, &#39;chest_xray&#39;) os.chdir(proj_dir) !pwd . In the Kaggle data directory page select New Notebook &gt; Three vertical dots, Copy API Command . #API key !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia . !unzip chest-xray-pneumonia . os.listdir(proj_dir) . The dataset is structured into training, val and test folders, each with sub-folders of NORMAL and PNEUMONIA images. . Data exploration . Image transforms . We will now prepare the data for reading into Pytorch as numpy arrays using DataLoaders. . Havig data augmentation is a good way to get extra training data for free. However, care must be taken to ensure that the transforms requested are likely to appear in the inference (or test set). . The images (RGB) are normalized using the mean [0.485,0.456,0.406] and standard deviation [0.229,0.224,0.225] of that used for the Imagenet data in the Resnet model, so that the new input images have the same distribution and mean as that used in the Resnet model. . I have set up two transforms dictionaries, one with and one without so it would be easy to plot images and compare. . imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) data_transforms = {&#39;train&#39; : T.Compose([ T.Resize(224), T.CenterCrop(224), T.RandomRotation(20), T.RandomHorizontalFlip(), T.ToTensor(), T.Normalize(*imagenet_stats, inplace=True) ]), &#39;test&#39; : T.Compose([ T.Resize(224), T.CenterCrop(224), T.ToTensor(), T.Normalize(*imagenet_stats, inplace=True) ]), &#39;val&#39; : T.Compose([ T.Resize(224), T.CenterCrop(224), T.ToTensor(), T.Normalize(*imagenet_stats, inplace=True) ])} data_no_transforms = {&#39;train&#39; : T.Compose([ T.ToTensor() ]), &#39;test&#39; : T.Compose([T.ToTensor() ]), &#39;val&#39; : T.Compose([T.ToTensor() ])} . Dataloaders . image_datasets = {x: datasets.ImageFolder(os.path.join(proj_dir, x), data_transforms[x]) for x in [&#39;train&#39;, &#39;val&#39;,&#39;test&#39;]} # Using the image datasets and the transforms, define the dataloaders dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=2) for x in [&#39;train&#39;, &#39;val&#39;,&#39;test&#39;]} dataset_sizes = {x: len(image_datasets[x]) for x in [&#39;train&#39;, &#39;val&#39;]} class_names= image_datasets[&#39;train&#39;].classes print(class_names) . op: [‘NORMAL’, ‘PNEUMONIA’] . Processed Images print(image_datasets[‘train’][0][0].shape) print(image_datasets[‘train’]) . op: op: torch.Size([3, 224, 224]) Dataset ImageFolder Number of datapoints: 5216 Root location: /content/drive/My Drive/jovian-xray/kaggle/chest_xray/train StandardTransform Transform: Compose( Resize(size=224, interpolation=PIL.Image.BILINEAR) CenterCrop(size=(224, 224)) RandomRotation(degrees=(-20, 20), resample=False, expand=False) RandomHorizontalFlip(p=0.5) ToTensor() Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ) . Visualisation . The image data is converted in to Numpy arrays and then treated with the mean and std so that we can view the images as seen by the model. . def imshow(img, title=None): img = img.numpy().transpose((1, 2, 0)) img = np.clip(img, 0, 1) plt.imshow(img) if title is not None: plt.title(title) def raw_imshow(img, title=None): img = img.numpy().transpose((1, 2, 0)) mean = np.array([0.485, 0.456, 0.406]) std = np.array([0.229, 0.224, 0.225]) img = std * img + mean img = np.clip(img, 0, 1) plt.imshow(img) if title is not None: plt.title(title) # Get a batch of training data images, classes = next(iter(dataloaders[&#39;train&#39;])) # Make a grid from batch out = torchvision.utils.make_grid(images) plt.figure(figsize=(8, 8)) raw_imshow(out, title=[class_names[x] for x in classes]) . . Transfer Learning model (Resnet34 model with our custom classifier) . The method of transfer learning is widely used to take advantage of the clever and hardworking chaps who have spent time to train a model on million+ images and save the trained model architecture and weights. . The Resnet34 model has been trained on the Imagenet database which has 1000 classes from trombones to toilet tissue. . Resnet34 has a top-most fully connected layer to predict 1000 classes. In our case we need only two so we will remove the last fc layer and add our own. . Have a look here for a good explanation of Resnet architectures. Briefly, the after each set of convolutions the input is added to the output. This helps to maintain the reslotion of the input, ie do not lose any features of the input model. . In a deep neural network the early layers capture generic features such as edges, texture and colour while the latter layers capture more specific features such as cats ears, eyes, elephant trunks and so on. . So our process is take the trained resnet architecture and weights, remove the head ie the last layers that are used to predict the 1000 classes and add our own tailored to the number of classes we want to predict, which in our case is two. . We will do a first pass of training where the weights of the resnet model are locked ie, ie we do not want to overwrite or lose those values which will mean more GPU expense for us. Then we will unfreeze the weights and run the entire model at our prefereed laerning rate. Note, idelaly we would like to unfreeze only specific layer, say layer 1 and layer 4, which I will cover in a separate blogpost. . Build and train your network . Load resnet-34 pre-trained network . model = models.resnet34(pretrained=True) . op: The tailed output of the model summary gives: . ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=512, out_features=1000, bias=True) ) In the Resnet34 architecture the final fully connected layer has in_features and 1000 out_features for the 1000 classes. But we need only two output classes. . So we add two linear layers to go from 512 RELU 256 and then 256 LOGSOFTMAX to 2 classes . Use a LogSoftmax for the final classification activation. . from collections import OrderedDict classifier = nn.Sequential(OrderedDict([ (‘fc1’, nn.Linear(2048, 1024)), (‘relu’, nn.ReLU()), (‘fc2’, nn.Linear(1024, 2)), (‘output’, nn.LogSoftmax(dim=1)) ])) . Replacing the pretrained model classifier with our classifier . model.fc = classifier . def freeze(model): # To freeze the residual layers for param in model.parameters(): param.requires_grad = False for param in model.fc.parameters(): param.requires_grad = True def unfreeze(self): # Unfreeze all layers for param in model.parameters(): param.requires_grad = True . freeze(model) . Count the trainable parameters to make sure we only include our new head. cp = count_parameters(model) print(f’{cp} trainable parameters in frozen model ‘) . op: 131842 trainable parameters in frozen model . Check: (512 * 256 + 256 bias) + (256 * 2 + 2 bias) . Setup the Training (and Validation) model . The dataset has training, val and tests which makes our lives a little bot easier ie we don’t have to do any data splitting and can set up specific transforms for each. . Training the model def train_model(model, criterion, optimizer, scheduler, num_epochs=10): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(1, num_epochs+1): print(&#39;Epoch {}/{}&#39;.format(epoch, num_epochs)) print(&#39;-&#39; * 10) # Each epoch has a training and validation phase for phase in [&#39;train&#39;, &#39;val&#39;]: if phase == &#39;train&#39;: scheduler.step() model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs, labels = inputs.to(device), labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == &#39;train&#39;): outputs = model(inputs) loss = criterion(outputs, labels) _, preds = torch.max(outputs, 1) # backward + optimize only if in training phase if phase == &#39;train&#39;: loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / dataset_sizes[phase] epoch_acc = running_corrects.double() / dataset_sizes[phase] print(&#39;{} Loss: {:.4f} Acc: {:.4f}&#39;.format( phase, epoch_loss, epoch_acc)) # deep copy the model if phase == &#39;val&#39; and epoch_acc &gt; best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) print() time_elapsed = time.time() - since print(&#39;Training complete in {:.0f}m {:.0f}s&#39;.format( time_elapsed // 60, time_elapsed % 60)) print(&#39;Best valid accuracy: {:4f}&#39;.format(best_acc)) # load best model weights model.load_state_dict(best_model_wts) return model . Check if a GPU is available . Any Deep Learning (neural network) model must be run on a GPU because the algoritms are tailored to exploit the parallel processing capabilities of these. So a quick check is made to see if a GPU exists so that data can be sent to this. Google COlab has a seeting under Edit &gt; Notebook Settings : Select None/GPU/TPU TPU is for Tensor Processing Unit which is even faster than GPU. . nThreads = 4 batch_size = 32 use_gpu = torch.cuda.is_available() train_on_gpu = torch.cuda.is_available() if not train_on_gpu: print(&#39;CUDA is not available. Training on CPU ...&#39;) else: print(&#39;CUDA is available! Training on GPU ...&#39;) device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) print(device) . Run the Training (and Validation) model . Use the Adam optimizer which is the preferred optimizer because it is adaptive and adds a momentum element to the gradient stepping. . Train a model with a pre-trained network . num_epochs = 10 if use_gpu: print (&quot;Using GPU: &quot;+ str(use_gpu)) model = model.cuda() . Use NLLLoss because our output is LogSoftmax criterion = nn.NLLLoss() . Adam optimizer with a learning rate . optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.0001) Decay LR by a factor of 0.1 every 5 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) . Train the frozen model where the bottom layers only are frozen . model_fit = train_model(model, criterion, optimizer(lr=0.001), exp_lr_scheduler, num_epochs=10) . Unfreeze the model and train some more . unfreeze(model) . Testing . Do validation on the test set . def test(model, dataloaders, device): model.eval() accuracy = 0 model.to(device) for images, labels in dataloaders[&#39;test&#39;]: images = Variable(images) labels = Variable(labels) images, labels = images.to(device), labels.to(device) output = model.forward(images) ps = torch.exp(output) equality = (labels.data == ps.max(1)[1]) accuracy += equality.type_as(torch.FloatTensor()).mean() print(&quot;Testing Accuracy: {:.3f}&quot;.format(accuracy/len(dataloaders[&#39;test&#39;]))) . test(model, dataloaders, device) . Save the checkpoint . model.class_to_idx = dataloaders[&#39;train&#39;].dataset.class_to_idx model.epochs = num_epochs checkpoint = {&#39;input_size&#39;: [2, 224, 224], &#39;batch_size&#39;: dataloaders[&#39;train&#39;].batch_size, &#39;output_size&#39;:2, &#39;state_dict&#39;: model.state_dict(), &#39;data_transforms&#39;: data_transforms, &#39;optimizer_dict&#39;:optimizer.state_dict(), &#39;class_to_idx&#39;: model.class_to_idx, &#39;epoch&#39;: model.epochs} save_fname= os.path.join(proj_dir, &#39;90_checkpoint.pth&#39;) torch.save(checkpoint, save_fname) . #Visualise the Training/Validation images . def visualize_model(model, num_images=6): was_training = model.training model.eval() images_so_far = 0 fig = plt.figure() with torch.no_grad(): for i, (inputs, labels) in enumerate(dataloaders[&#39;test&#39;]): inputs = inputs.to(device) labels = labels.to(device) outputs = model(inputs) _, preds = torch.max(outputs, 1) for j in range(inputs.size()[0]): images_so_far += 1 ax = plt.subplot(num_images//2, 2, images_so_far) ax.axis(&#39;off&#39;) ax.set_title(&#39;predicted: {}&#39;.format(class_names[preds[j]])) imshow(inputs.cpu().data[j]) if images_so_far == num_images: model.train(mode=was_training) return model.train(mode=was_training) . visualize_model(model_ft) . print (predict(&#39;chest_xray/test/NORMAL/NORMAL2-IM-0348-0001.jpeg&#39;, loaded_model)) . (array([0.96704894, 0.03295105], dtype=float32), [‘NORMAL’, ‘PNEUMONIA’]) . The NORMAL probablility is much larger than the one for pneumonia. . img = os.path.join(proj_dir, &#39;test/NORMAL/NORMAL2-IM-0347-0001.jpeg&#39;) p, c = predict(img, loaded_model) view_classify(img, p, c, class_names) . . img = os.path.join(proj_dir, &#39;test/PNEUMONIA/person85_bacteria_421.jpeg&#39;) p, c = predict(img, loaded_model) view_classify(img, p, c, class_names) . . Lessons Learned . There are five methods to reduce model overfitting. Overfitting results when the model fits very well to the training data (low error) but not very well to the validation data (high error). These are: Get more data Data augmentation Generalizable architectures Regularisation Reduce architecture complexity . | Undertaking an online course like the Jovian Zero to Gans has been an excellent opportunity to immerse myself in Machine Learning. Taking part in the competition (which is ongoing) and writing this blog on the X-ray dataset has helped me to better understand important concepts such as Dataloaders, learning rate, batch size, optimizers and loss functions. . | Thank you to Aakash the course instructor and the rest pf the Jovian team for the efforts in helping us to better understand such an exciting paradigm. | . I will keep updating this blog with better images from Colab (nned to set the background to light). . . .",
            "url": "https://onpointai.github.io/onpointai/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html",
            "relUrl": "/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html",
            "date": " • Jun 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Tensorflow & Pytorch comparison with CIFAR10",
            "content": "About . The third assigment of the course Pytorch Zero to GANS run by JOVIAN.ML is to go through a simple classification problem using the CIFAR10 dataset. The course uses Pytorch and as an option course attendees were asked to use Tensorflow to repeat the task. . The training was done on Google Colab with GPU. The good thing with running on Colab (and Binder, Kaggle to name a few others) is there is not much setup involved. Import the required libraries and off you go! . The Tensorflow version will be done first followed by the Pytorch version. The course assignment was in Pytorch (as the course title suggests) so the TF example was made to match that setup. . Colab setup . You will have to use your own API credentials. . from google.colab import drive drive.mount(&#39;/content/drive&#39;, force_remount=True) import os root_dir = &#39;/content/drive/My Drive/Colab Notebooks/jovian/&#39; . Mounted at /content/drive . Tensorflow/Keras . Import libraries . from __future__ import print_function import tensorflow as tf import keras from keras.datasets import cifar10 from keras.preprocessing.image import ImageDataGenerator from keras.models import Sequential from keras.layers import Dense, Dropout, Activation, Flatten from keras.utils import to_categorical import os import numpy as np from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt %matplotlib inline . Data augmentation not used because this was not used in the Pytorch example below. ** . batch_size=128 epochs=20 data_augmentation=False . (X,y), (x_test,y_test) = cifar10.load_data() . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 4s 0us/step . Explore the CIFAR10 dataset . # collapse-hide print(X.shape) print(y.shape) print(x_test.shape) print(y_test.shape) . . (50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1) . y_squeeze = np.squeeze(y) . # Create a classes list classes = [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;] . #unique classes unique_classes = np.unique(y) num_classes = len(unique_classes) unique_classes . array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8) . unique, count = np.unique(y, return_counts=True) . for i in range(len(unique)): print(f&#39; class {classes[i] } has {count[i].item()} images&#39;) . class airplane has 5000 images class automobile has 5000 images class bird has 5000 images class cat has 5000 images class deer has 5000 images class dog has 5000 images class frog has 5000 images class horse has 5000 images class ship has 5000 images class truck has 5000 images . #collapse-hide fig = plt.figure(figsize=(6,6)) for i in range(9): plt.subplot(3,3,i+1) plt.imshow(X[i]) plt.show() . . Prepare the data for training . The Pytorch assignment used a 10% split of the training set for the validation set done using the randome_split Pytorch utility. So I will use scikitlearn&#39;s train_test_split to do the same the same on the training set. . x_train, x_val, y_train, y_val= train_test_split(X, y, test_size=0.1, random_state=42) . x_train=x_train.astype(&#39;float32&#39;)/255. x_val=x_val.astype(&#39;float32&#39;)/255. . y_train = to_categorical(y_train, num_classes) y_val = to_categorical(y_val, num_classes) print(x_train.shape) print(y_train.shape) . (45000, 32, 32, 3) (45000, 10) . y_train = np.squeeze(y_train) y_val=np.squeeze(y_val) . Set up a simple Keras model . model = Sequential() model.add(Flatten(input_shape=(32,32,3))) model.add(Dense(32,activation = &#39;relu&#39;)) model.add(Dense(num_classes)) model.add(Activation(&#39;softmax&#39;)) . model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_1 (Flatten) (None, 3072) 0 _________________________________________________________________ dense_1 (Dense) (None, 32) 98336 _________________________________________________________________ dense_2 (Dense) (None, 10) 330 _________________________________________________________________ activation_1 (Activation) (None, 10) 0 ================================================================= Total params: 98,666 Trainable params: 98,666 Non-trainable params: 0 _________________________________________________________________ . The number of model parameters must match that of Pytorch. . opt = keras.optimizers.SGD(learning_rate=1e-3) . model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=opt, metrics = [&#39;accuracy&#39;]) . H = model.fit(x_train, y_train, batch_size=batch_size, epochs = 20, validation_data=(x_val,y_val)) . print(H.history.keys()) . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . plt.title(&quot;ACCURACY&quot;) plt.plot(H.history[&#39;accuracy&#39;], label=&#39;train_acc&#39;) plt.plot(H.history[&#39;val_accuracy&#39;], label = &#39;val_acc&#39;) plt.legend() plt.show() . Evaluate the model on Test data . x_test =x_test.astype(&#39;float32&#39;)/255. y_test = to_categorical(y_test, num_classes) . y_test.shape . (10000, 10) . model.evaluate(x_test, y_test) . 10000/10000 [==============================] - 1s 94us/step . [1.8181508338928223, 0.3619999885559082] . . Pytorch . This is Assignment 03 modified for blogging purposes. . Import libraries . import torch import torchvision import numpy as np import matplotlib.pyplot as plt import torch.nn as nn import torch.nn.functional as F from torchvision.datasets import CIFAR10 from torchvision.transforms import ToTensor from torchvision.utils import make_grid from torch.utils.data.dataloader import DataLoader from torch.utils.data import random_split %matplotlib inline . # Project name used for jovian.commit project_name = &#39;03-cifar10-feedforward&#39; . proj_dir = os.path.join(root_dir, project_name) proj_dir . &#39;/content/drive/My Drive/Colab Notebooks/jovian/03-cifar10-feedforward&#39; . Exploring the CIFAR10 dataset . dataset = CIFAR10(root=proj_dir, download=True, transform=ToTensor()) test_dataset = CIFAR10(root=proj_dir, train=False, transform=ToTensor()) . Files already downloaded and verified . dataset_size = len(dataset) dataset_size . 50000 . test_dataset_size = len(test_dataset) test_dataset_size . 10000 . classes = dataset.classes classes . [&#39;airplane&#39;, &#39;automobile&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;] . num_classes = len(dataset.classes) num_classes . 10 . Note that this dataset consists of 3-channel color images (RGB). Let us look at a sample image from the dataset. matplotlib expects channels to be the last dimension of the image tensors (whereas in PyTorch they are the first dimension), so we&#39;ll the .permute tensor method to shift channels to the last dimension. Let&#39;s also print the label for the image. . The number of images belonging to each class . Credit . #get the label of the dataset using the [1] index img, label = dataset[1] label_of_image_1 = dataset[1][1] title= str(label_of_image_1) + &#39; is a &#39; + classes[label_of_image_1] plt.imshow(img.permute(1,2,0)) plt.title(title) plt.show() . label_of_train_images=[] for i in range(len(dataset)): label_of_train_image = dataset[i][1] label_of_train_images.append(label_of_train_image) . num_unique_train_labels = np.unique(label_of_train_images) . uniq_image_count = torch.stack([(torch.tensor(label_of_train_images)==i).sum() for i in num_unique_train_labels]) . for i in range(len(uniq_image_count)): print(f&#39; class {classes[i] } has {uniq_image_count[i].item()} images&#39;) . class airplane has 5000 images class automobile has 5000 images class bird has 5000 images class cat has 5000 images class deer has 5000 images class dog has 5000 images class frog has 5000 images class horse has 5000 images class ship has 5000 images class truck has 5000 images . Prepare the data for training . torch.manual_seed(43) val_size = 5000 train_size = len(dataset) - val_size . Let&#39;s use the random_split method to create the training &amp; validation sets . train_ds, val_ds = random_split(dataset, [train_size, val_size]) len(train_ds), len(val_ds) . (45000, 5000) . We can now create data loaders to load the data in batches. . batch_size=128 . train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True) val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True) test_loader = DataLoader(test_dataset, batch_size*2, num_workers=4, pin_memory=True) . Let&#39;s visualize a batch of data using the make_grid helper function from Torchvision. . #collapse-hide viz_loader = DataLoader(train_ds, 9, shuffle=True, num_workers=4, pin_memory=True) for images, _ in viz_loader: plt.figure(figsize=(6,6)) plt.axis(&#39;off&#39;) plt.imshow(make_grid(images, nrow=3).permute((1, 2, 0))) break . . Base Model class &amp; Training on GPU . Let&#39;s create a base model class, which contains everything except the model architecture i.e. it wil not contain the __init__ and __forward__ methods. We will later extend this class to try out different architectures. In fact, you can extend this model to solve any image classification problem. . def accuracy(outputs, labels): _, preds = torch.max(outputs, dim =1) return torch.tensor(torch.sum(preds== labels).item()/len(preds)) . class ImageClassificationBase(nn.Module): def training_step(self, batch): images, labels = batch out = self(images) # Generate predictions loss = F.cross_entropy(out, labels) # Calculate loss return loss def validation_step(self, batch): images, labels = batch out = self(images) # Generate predictions loss = F.cross_entropy(out, labels) # Calculate loss #change from cross_entropy acc = accuracy(out, labels) # Calculate accuracy return {&#39;val_loss&#39;: loss, &#39;val_acc&#39;: acc} def validation_epoch_end(self, outputs): batch_losses = [x[&#39;val_loss&#39;] for x in outputs] epoch_loss = torch.stack(batch_losses).mean() # Combine losses batch_accs = [x[&#39;val_acc&#39;] for x in outputs] epoch_acc = torch.stack(batch_accs).mean() # Combine accuracies return {&#39;val_loss&#39;: epoch_loss.item(), &#39;val_acc&#39;: epoch_acc.item()} def epoch_end(self, epoch, result): print(&quot;Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}&quot;.format(epoch, result[&#39;val_loss&#39;], result[&#39;val_acc&#39;])) . We can also use the exact same training loop as before. I hope you&#39;re starting to see the benefits of refactoring our code into reusable functions. . def evaluate(model, val_loader): outputs = [model.validation_step(batch) for batch in val_loader] return model.validation_epoch_end(outputs) def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD): history = [] optimizer = opt_func(model.parameters(), lr) for epoch in range(epochs): # Training Phase for batch in train_loader: loss = model.training_step(batch) loss.backward() optimizer.step() optimizer.zero_grad() # Validation phase result = evaluate(model, val_loader) model.epoch_end(epoch, result) history.append(result) return history . Finally, let&#39;s also define some utilities for moving out data &amp; labels to the GPU, if one is available. . Let us also define a couple of helper functions for plotting the losses &amp; accuracies. . def plot_losses(history): losses = [x[&#39;val_loss&#39;] for x in history] plt.plot(losses, &#39;-x&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;loss&#39;) plt.title(&#39;Loss vs. No. of epochs&#39;); . def plot_accuracies(history): accuracies = [x[&#39;val_acc&#39;] for x in history] plt.plot(accuracies, &#39;-o&#39;) plt.xlabel(&#39;epoch&#39;) plt.ylabel(&#39;accuracy&#39;) plt.title(&#39;Pytorch Accuracy vs. No. of epochs&#39;); . Let&#39;s move our data loaders to the appropriate device. . input_size = 3*32*32 hidden_size = 32 output_size = 10 . class CIFAR10Model(ImageClassificationBase): def __init__(self): super().__init__() #dd hidden layer self.linear1 = nn.Linear(input_size, hidden_size) #output layer self.linear2 = nn.Linear(hidden_size, output_size) def forward(self, xb): # Flatten images into vectors out = xb.view(xb.size(0), -1) # Apply layers &amp; activation functions out = self.linear1(out) out = F.relu(out) out = self.linear2(out) return out . You can now instantiate the model, and move it the appropriate device. . #USING A GPU torch.cuda.is_available() def get_default_device(): &quot;&quot;&quot;Pick GPU if available, else CPU&quot;&quot;&quot; if torch.cuda.is_available(): return torch.device(&#39;cuda&#39;) else: return torch.device(&#39;cpu&#39;) device = get_default_device() print(device) def to_device(data, device): &quot;&quot;&quot;Move tensor(s) to chosen device&quot;&quot;&quot; if isinstance(data, (list,tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking=True) for images, labels in train_loader: print(images.shape) images = to_device(images, device) print(images.device) break . cuda torch.Size([128, 3, 32, 32]) cuda:0 . model = to_device(CIFAR10Model(), device) . print(model.parameters) for t in model.parameters(): print(t.shape) . &lt;bound method Module.parameters of CIFAR10Model( (linear1): Linear(in_features=3072, out_features=32, bias=True) (linear2): Linear(in_features=32, out_features=10, bias=True) )&gt; torch.Size([32, 3072]) torch.Size([32]) torch.Size([10, 32]) torch.Size([10]) . Before you train the model, it&#39;s a good idea to check the validation loss &amp; accuracy with the initial set of weights. . #collapse-hide pytorch_total_params = sum(p.numel() for p in model.parameters()) print(&#39;Total number of parameters: &#39;,pytorch_total_params) pytorch_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) print(&#39;Trainable parameters: &#39;, pytorch_trainable_params) print(&#39;layers + activations&#39;,len(list(model.parameters()))) . . Total number of parameters: 98666 Trainable parameters: 98666 layers + activations 4 . The number of model parameters matches that of TF/Keras . class DeviceDataLoader(): &quot;&quot;&quot;Wrap a dataloader to move data to a device&quot;&quot;&quot; def __init__(self, dl, device): self.dl = dl self.device = device def __iter__(self): &quot;&quot;&quot;Yield a batch of data after moving it to device&quot;&quot;&quot; for b in self.dl: yield to_device(b, self.device) def __len__(self): &quot;&quot;&quot;Number of batches&quot;&quot;&quot; return len(self.dl) . train_loader = DeviceDataLoader(train_loader, device) val_loader = DeviceDataLoader(val_loader, device) test_loader= DeviceDataLoader(test_loader, device) . for xb, yb in val_loader: print(&#39;xb.device:&#39;, xb.device) xb = xb.view(xb.size(0), -1) break . xb.device: cuda:0 . history = [evaluate(model, val_loader)] history . [{&#39;val_acc&#39;: 0.09672564268112183, &#39;val_loss&#39;: 2.308856248855591}] . history = fit(20, 1e-3, model, train_loader, val_loader) history . Plot the losses and the accuracies to check if you&#39;re starting to hit the limits of how well your model can perform on this dataset. You can train some more if you can see the scope for further improvement. . plot_accuracies(history) . Finally, evaluate the model on the test dataset report its final performance. . evaluate(model, test_loader) . {&#39;val_acc&#39;: 0.33740234375, &#39;val_loss&#39;: 1.88686203956604} . Results . Accuracy Tensorflow/Keras : 0.3619999885559082 Pytorch: 0.33740234375 . The differences could be due to very little training and hence lack of convergence of the solution, the randomness of the weights intialisation and differences in the library implementations in TF and Pytorch and other stuff I am not aware of ;) . Concluding comments . This exercise was not to get an exact match of accuracy but to demonstrate the constructs between TF and Pytorch. . The Jovian course Pytorch Zero to GANS is a great introduction to Machine Learning. I am enjoying . Collaborating with others | Working through examples | Finishing assignments and submitting for approval | Blogging about my experiences | Staying enthusiastic about ML! | . Thanks and appreciation to: . Jeremy Howard and Hamel Hussain for the Fastpages framework in which this blog is written as a Jupyter notebook. | .",
            "url": "https://onpointai.github.io/onpointai/jovian/tensorflow/keras/pytorch/fastpages/jupyter/2020/06/12/tfkeras-pytorch-cifar10.html",
            "relUrl": "/jovian/tensorflow/keras/pytorch/fastpages/jupyter/2020/06/12/tfkeras-pytorch-cifar10.html",
            "date": " • Jun 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Loss Functions",
            "content": "About . A Loss Function is a function that calculates a single real number which indicates how far a prediction is from the actual. During the creation of a Machine Learning model the loss is minimised by using some type of algorithm also known as an Optimiser. The Optimiser optimises the result by reducing the loss. In general, the loss is calculated over a number of input examples (batch). . Tutorial Overiew . JASON BROWNLEE MACHINE LEARNING MASTERY . This tutorial is divided into three parts; they are: . Regression Loss Functions . Mean Squared Error Loss - default - distribution of target is gaussian - square of error means larger errors penalised more than smaller ones (Keras mse or mean_squared_error) . Mean Squared Logarithmic Error Loss . Mean Absolute Error Loss - distribution of target variable is mainly gaussian but may heave outliers (keras - mean_absolute_error) . Binary Classification Loss Functions - targets are either of two labels . **Binary Cross-Entropy (keras - binary_cross_entropy) . Hinge Loss - for use with SVM - binay classification where target values are in the set {-1,1} encourages examples to have the correct sign - assigning more errorwhen thee is a differene in sign between actual/predicted class values . Squared Hinge Loss - square of hinge loss - hass effect of smoothing the surfaceof the error function and making it easier to work with numerically . Multi-Class Classification Loss Functions - targets can belong to one of many labels or classes - predict probability of example beloging to each known class . Multi-Class Cross-Entropy Loss - use this first - calculate a score that summarizes the ave diff between actual and predicted probability distributions for all classes - keras -categorical_cross_entropy . Sparse Multiclass Cross-Entropy Loss - large nb of labels- eg words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. No one-hot encoding keras - sparse_categorical_crossentropy . Kullback Leibler Divergence Loss KL divergence - measure of how one probability distribution differs from a baseline distribution KLdiv of 0 suggest distributions are identical calculates how much information is lost if the predicted target distributon is use to approximate the desired target probablity distribuion KLd more commonly used when using models that learn to approximate a more complex fn than simply multi-class ie autoencoding used for learning a dense feature representation under a model that must construct the original input . Regression loss functions . #&#39;/Users/dexterdsilva/Documents/Developer/MachineLearning/brownlee_mlm&#39; import os os.path.abspath(&#39;&#39;) . &#39;/Users/dexterdsilva/Documents/Developer/MachineLearning/brownlee_mlm&#39; . from sklearn.datasets import make_regression from sklearn.preprocessing import StandardScaler from keras.models import Sequential from keras.layers import Dense from keras.optimizers import SGD import matplotlib.pyplot as plt . Using TensorFlow backend. . import keras print(keras.__version__) import tensorflow as tf print(tf.__version__) . 2.3.1 2.1.0 . #generate regression dataset #20 X,y=make_regression(n_samples=1000, n_features=20,noise=0.1, random_state=1) . X[0] . array([ 0.58372668, 0.78593639, -0.17187155, 0.66928708, 1.67181016, 0.59831823, 1.49807611, 0.27925069, -0.31705821, -0.41961259, -0.21796143, 0.81186707, -0.79215259, 0.56621046, 0.97473625, -0.8223744 , 1.03007179, -0.67945508, -0.21540618, 1.03118947]) . print(y.shape) f&#39;{y[0]}&#39; print(type(y)) . (1000,) &lt;class &#39;numpy.ndarray&#39;&gt; . X=StandardScaler().fit_transform(X) . X[0] . array([ 0.64516745, 0.76000873, -0.18010938, 0.65740427, 1.65197553, 0.56574009, 1.50368462, 0.30416606, -0.29634854, -0.37041198, -0.22745535, 0.72392625, -0.76421488, 0.55675061, 0.96630152, -0.82406123, 0.95038766, -0.76479647, -0.2088517 , 1.04702956]) . y=StandardScaler().fit_transform(y.reshape(len(y),1))[:,0] . n_train=500 trainX, testX = X[:n_train,:], X[n_train:,:] trainy, testy = y[:n_train], y[n_train:] . model= Sequential() model.add(Dense(25, input_dim=20,activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(1, activation=&#39;linear&#39;)) . opt=SGD(lr=0.01, momentum=0.9) . Mean Squared Error Loss . model.compile(loss=&#39;mse&#39;, optimizer = opt) . h=model.fit(trainX, trainy, validation_data=(testX,testy), epochs=100, verbose=0) . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;loss&#39;]) . fig= plt.figure(figsize=(5,5)) plt.plot(h.history[&#39;loss&#39;],label=&#39;train_loss&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test_loss&#39;) plt.legend() plt.show() . Mean Squared Logarithmic Error Loss . model=Sequential() model.add(Dense(25, input_dim=20, activation =&#39;relu&#39;, kernel_initializer = &#39;he_uniform&#39;)) model.add(Dense(1, activation = &#39;linear&#39;)) opt=SGD(lr=0.1, momentum=0.9) model.compile(loss=&#39;mean_squared_logarithmic_error&#39;, optimizer=opt, metrics=[&#39;mse&#39;]) . print(trainX.shape,&#39; &#39;,trainy.shape,&#39; &#39;, testX.shape,&#39; &#39;, testy.shape) . (500, 20) (500,) (500, 20) (500,) . h = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0) . # evaluate the model _,train_mse=model.evaluate(trainX, trainy, verbose=0) _,test_mse=model.evaluate(testX, testy, verbose=0) . print(&#39;Train: {:.2f} Test {:.2f}&#39;.format(train_mse, test_mse)) . Train: 0.31 Test 0.37 . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_mse&#39;, &#39;loss&#39;, &#39;mse&#39;]) . fig=plt.figure(figsize=(10,10)) plt.subplot(211) plt.title(&#39;Loss&#39;) plt.plot(h.history[&#39;loss&#39;],label=&#39;Training Loss&#39;) plt.plot(h.history[&#39;val_loss&#39;],label = &#39;Test Loss&#39;) plt.legend() plt.subplot(212) plt.title(&#39;Mean Squared Error&#39;) plt.plot(h.history[&#39;mse&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_mse&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Mean Absolute Error Loss . X,y=make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1) . print(X.shape, y.shape,&#39; &#39;, type(y)) plt.scatter(X[:,1],y) . (1000, 20) (1000,) &lt;class &#39;numpy.ndarray&#39;&gt; . &lt;matplotlib.collections.PathCollection at 0x13e0af090&gt; . len(y) . 1000 . X=StandardScaler().fit_transform(X) y=StandardScaler().fit_transform(y.reshape(len(y),1)) . n_train=500 trainX, testX = X[:n_train,:],X[n_train:,:] trainy, testy = y[:n_train], y[n_train:] . model= Sequential() model.add(Dense(25,input_dim=20, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(1, activation=&#39;linear&#39;)) opt=SGD(lr=0.01, momentum=0.9) model.compile(loss=&#39;mean_absolute_error&#39;, optimizer=opt,metrics=[&#39;mse&#39;]) . h=model.fit(trainX, trainy,validation_data=(testX, testy), epochs=100, verbose=1) . _,train_mse=model.evaluate(trainX, trainy, verbose=1) . 500/500 [==============================] - 0s 18us/step . _,test_mse=model.evaluate(testX, testy, verbose=1) . 500/500 [==============================] - 0s 19us/step . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_mse&#39;, &#39;loss&#39;, &#39;mse&#39;]) . print(&#39;Train: {:.4f} Test:{:.4f}&#39;.format(train_mse, test_mse)) . Train: 0.0042 Test:0.0036 . fig=plt.figure(figsize=(10,10)) plt.subplot(121) plt.title(&#39;LOSS&#39;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&#39;MSE&#39;) plt.plot(h.history[&#39;mse&#39;],label=&#39;train&#39;) plt.plot(h.history[&#39;val_mse&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Classification Loss Functions . from sklearn.datasets import make_circles from sklearn.preprocessing import StandardScaler from keras.models import Sequential from keras.layers import Dense from keras.optimizers import SGD from numpy import where import matplotlib.pyplot as plt %matplotlib inline . X,y = make_circles(n_samples=1000,noise=0.1, random_state=1) for i in range(2): samples_idx= where(y==i) plt.scatter(X[samples_idx,0], X[samples_idx,1], label=str(i)) plt.legend() plt.show() . print(X[:5]) print(y[:5]) . [[ 0.92787748 -0.04521731] [-0.54303182 -0.75444674] [ 0.9246533 -0.71492522] [-0.10217077 -0.89283523] [-1.01719242 0.24737775]] [1 1 0 0 0] . n_train=500 trainX,testX=X[:n_train,:],X[:n_train] trainy,testy = y[:n_train], y[:n_train] . print(trainX.shape ,&#39; &#39;, trainy.shape) . (500, 2) (500,) . Binary Cross Entropy . model=Sequential() model.add(Dense(50, input_dim=2, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) opt=SGD(lr=0.01, momentum=0.9) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) . h=model.fit(trainX, trainy, validation_data=(testX,testy), epochs=200,verbose=1) . #_,train_acc=model.evaluate(trainX,trainy, verbose=1) . 500/500 [==============================] - 0s 44us/step . _,test_acc=model.evaluate(testX, testy,verbose=1) . 500/500 [==============================] - 0s 25us/step . print(&#39;Train: {:.4f} Test:{:.4f}&#39;.format(train_acc, test_acc)) . Train: 0.8360 Test:0.8360 . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . fig=plt.figure(figsize=(8,4)) plt.subplot(121) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&quot;ACCURACY&quot;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Hinge Loss . y[where(y==0)]= -1 . X,y = make_circles(n_samples=1000, noise=0.1, random_state=1) y[where(y==0)]= -1 . n_train=500 trainX,testX= X[:n_train,:],X[n_train:,:] trainy,testy=y[:n_train],y[n_train:] . model=Sequential() model.add(Dense(50,input_dim=2,activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(1, activation=&#39;tanh&#39;)) opt=SGD(lr=0.01, momentum=0.9) model.compile(loss=&#39;hinge&#39;, optimizer=opt,metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential_14&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_24 (Dense) (None, 50) 150 _________________________________________________________________ dense_25 (Dense) (None, 1) 51 ================================================================= Total params: 201 Trainable params: 201 Non-trainable params: 0 _________________________________________________________________ . h=model.fit(trainX,trainy,validation_data=(testX,testy), epochs=200, verbose=1) . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . _,train_acc=model.evaluate(trainX,trainy, verbose=1) . 500/500 [==============================] - 0s 21us/step . _,test_acc=model.evaluate(testX,testy, verbose= 1) . 500/500 [==============================] - 0s 19us/step . print(&#39;Train: {:.4f} Test: {:.4f}&#39;.format(train_acc, test_acc)) . Train: 0.8380 Test: 0.8380 . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . fig =plt.figure(figsize=(10,5)) plt.subplot(1,2,1) plt.title(&#39;ACCURACY&#39;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label = &#39;test&#39;) plt.legend() plt.subplot(1,2,2) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Squared Hinge Loss . X,y = make_circles(n_samples=1000, noise=0.1, random_state=1) y[where(y==0)]= -1 . n_train=500 trainX,testX = X[:n_train,:],X[n_train:,:] trainy,testy=y[:n_train],y[n_train:] . collapse-hide model = Sequential() model.add(Dense(50, input_dim=2, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(1, activation=&#39;tanh&#39;)) opt=SGD(lr=0.01, momentum=0.9) model.compile(loss=&#39;squared_hinge&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) h = model.fit(trainX,trainy,validation_data=(testX,testy), epochs=200, verbose=1) . print(model.metrics_names) res_train = model.evaluate(trainX,trainy) res_test = model.evaluate(testX, testy) . [&#39;loss&#39;, &#39;accuracy&#39;] 500/500 [==============================] - 0s 18us/step 500/500 [==============================] - 0s 18us/step . print(&#39;Traina cc:{:.4f} Test acc:{:.4f}&#39;.format(res_train[1], res_test[1])) . Traina cc:0.6820 Test acc:0.6660 . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . fig=plt.figure(figsize=(8,2)) plt.subplot(121) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&quot;ACCURACY&quot;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Multi-class classification . Multi-class cross-entropy . from sklearn.datasets import make_blobs X,y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2) . print(X[:5], &#39; &#39;, y[:5]) . [[ 0.48719811 -0.43160548] [ -1.48958879 -3.47915742] [ -2.06250444 -7.73300419] [ -0.51369303 -10.31546366] [ 0.56240126 -2.18246169]] [2 2 2 0 1] . for i in range(3): samples_idx = where(y==i) plt.scatter(X[samples_idx,0], X[samples_idx,1], label=str(i)) plt.legend() plt.show() . from keras.utils import to_categorical X,y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2) y = to_categorical(y) #NO ONEHOT ENCODING FOR SPARSE n_train=500 trainX,testX = X[:n_train,:],X[n_train:,:] trainy,testy=y[:n_train],y[n_train:] . print(trainy.shape) print(y[:5]) . (500, 3) [[0. 0. 1.] [0. 0. 1.] [0. 0. 1.] [1. 0. 0.] [0. 1. 0.]] . model = Sequential() model.add(Dense(50, input_dim=2, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(3, activation=&#39;softmax&#39;)) opt=SGD(lr=0.01, momentum=0.9) #model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;], ) h = model.fit(trainX,trainy,validation_data=(testX,testy), epochs=200, verbose=1) . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . res_train = model.evaluate(trainX,trainy) res_test = model.evaluate(testX, testy) print(&#39;Train acc:{:.4f} Test acc:{:.4f}&#39;.format(res_train[1], res_test[1])) . 500/500 [==============================] - 0s 42us/step 500/500 [==============================] - 0s 33us/step Train acc:0.8320 Test acc:0.8260 . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . #collapse-hide fig=plt.figure(figsize=(8,2)) plt.subplot(121) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&quot;ACCURACY&quot;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label=&#39;test&#39;) plt.legend() plt.show() . . Sparse multi-class cross-entropy . from keras.utils import to_categorical X,y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2) y = to_categorical(y) NO ONEHOT ENCODING FOR SPARSE n_train=500 trainX,testX = X[:n_train,:],X[n_train:,:] trainy,testy=y[:n_train],y[n_train:] print(trainy.shape) print(y[:5]) . (500,) [2 2 2 0 1] . model = Sequential() model.add(Dense(50, input_dim=2, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(3, activation=&#39;softmax&#39;)) opt=SGD(lr=0.01, momentum=0.9) #model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;], ) h = model.fit(trainX,trainy,validation_data=(testX,testy), epochs=200, verbose=1) #res_train = model.evaluate(trainX,trainy) #res_test = model.evaluate(testX, testy) #print(&#39;Train acc:{:.4f} Test acc:{:.4f}&#39;.format(res_train[1], res_test[1])) . fig=plt.figure(figsize=(8,2)) plt.subplot(121) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&quot;ACCURACY&quot;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label=&#39;test&#39;) plt.legend() plt.show() . Kullback Leibler Divergence . Measure of how one probablity distribution differs form another KL Div = 0 suggest dist are identical autoenoder used for learning dense featue representation under a model that must reconstruct the original input . #collapse-hide from sklearn.datasets import make_blobs from keras.layers import Dense from keras.models import Sequential from keras.optimizers import SGD from keras.utils import to_categorical import matplotlib.pyplot as plt . . X,y = make_blobs(n_samples=1000,centers=3,n_features=2, cluster_std=2, random_state=2) . y[:5] . array([2, 2, 2, 0, 1]) . y = to_categorical(y) . y[:5] . array([[0., 0., 1.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.], [0., 1., 0.]], dtype=float32) . n_train=500 trainX,testX = X[:n_train,:],X[n_train:,:] trainy, testy = y[n_train:],y[:n_train] . #collapse-hide model= Sequential() model.add(Dense(50, input_dim=2, activation=&#39;relu&#39;, kernel_initializer=&#39;he_uniform&#39;)) model.add(Dense(3, activation=&#39;softmax&#39;)) opt=SGD(lr=0.01, momentum=0.9) model.compile(loss=&#39;kullback_leibler_divergence&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;]) . . h = model.fit(trainX ,trainy, validation_data=(testX, testy), epochs =100, verbose=1) . model.evaluate(trainX,trainy,verbose=1) . 500/500 [==============================] - 0s 49us/step . [1.0587191171646118, 0.41600000858306885] . model.metrics_names . [&#39;loss&#39;, &#39;accuracy&#39;] . h.history.keys() . dict_keys([&#39;val_loss&#39;, &#39;val_accuracy&#39;, &#39;loss&#39;, &#39;accuracy&#39;]) . fig=plt.figure(figsize=(8,2)) plt.subplot(121) plt.title(&quot;LOSS&quot;) plt.plot(h.history[&#39;loss&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_loss&#39;], label=&#39;test&#39;) plt.legend() plt.subplot(122) plt.title(&quot;ACCURACY&quot;) plt.plot(h.history[&#39;accuracy&#39;], label=&#39;train&#39;) plt.plot(h.history[&#39;val_accuracy&#39;], label=&#39;test&#39;) plt.legend() plt.show() . FIN .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/jupyter/2020/06/05/lossfunctions.html",
            "relUrl": "/ai/ml/jupyter/2020/06/05/lossfunctions.html",
            "date": " • Jun 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Sports Video Classification",
            "content": "Overview . The simplest way to classify the type of video is by examining individual frames of the video ie treat individual frames as images, classify the images and then do some sort of averaging or smoothing over a few frames to predict the current display. . Once we understand what goes on here we can explore the other methods which take up more setup and compute. . The main package here is OpenCV, a library aimed at real-time computer vision tasks. . The dataset used was a collection of images curated using Google image search. . ├── Sports-Type-Classifier │ ├── data │ │ ├── badminton [938 entries] │ │ ├── baseball [746 entries] │ │ ├── basketball [495 entries] │ │ ├── boxing [705 entries] │ │ ├── chess [481 entries] │ │ ├── cricket [715 entries] │ │ ├── fencing [635 entries] │ │ ├── football [799 entries] │ │ ├── formula1 [687 entries] │ │ ├── gymnastics [719 entries] │ │ ├── hockey [572 entries] │ │ ├── ice_hockey [715 entries] │ │ ├── kabaddi [454 entries] │ │ ├── motogp [679 entries] │ │ ├── shooting [536 entries] │ │ ├── swimming [689 entries] │ │ ├── table_tennis [713 entries] │ │ ├── tennis [718 entries] │ │ ├── volleyball [713 entries] │ │ ├── weight_lifting [577 entries] │ │ ├── wrestling [611 entries] │ │ ├── wwe [671 entries] . Setup . The repo contains the data, Colab Jupyter notebooks and trained weights for different training regimes. . Results . . . . . . . .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/video%20classification/2020/05/21/videoclassification.html",
            "relUrl": "/ai/ml/video%20classification/2020/05/21/videoclassification.html",
            "date": " • May 21, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Multi-label Image Classification",
            "content": "Overview . This is a tutorial from Analytics Vidhya . The dataset of images contain more than two categories ie it is not a simple either/or . Each image in the dataset can contain only one category . Example: A dataset containing images such as dog, cat, rabbit, parrot Each image contains only dog, cat, parrot rabbit . . The above is know as multi-label image classification. . Question: Can we predict the genre of a movie by looking at the movie poster? And ofcourse a movie can belong to more than one genre. . The key is in the output layer - use a sigmoid activation instead of softmax. With Softmax as the probablity of one increases the probability of the other classses decrease (becuase the sum must equal 1). With Sigmoid however the probabilities are independent of each other. So with sigmoid the architecture will internally create N models where N is the number of classes. Cool huh?! . Setup . For details of the model and data see repo . Note: No attempt has been made to finetune the architecture and reduce the amount of overfitting and hence get a better training/validation loss. . Results . . . .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/multi-label%20image%20classification/2020/05/19/multilabelimageclass.html",
            "relUrl": "/ai/ml/multi-label%20image%20classification/2020/05/19/multilabelimageclass.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Style Transfer using Neural Networks",
            "content": "Overview . This method renders the contents of a Original image in the style of a Reference image. For example in an image of a cityscape the contents can be considered as the hard edges of the buildings. The Reference image can contain lots of swirly patters and colours which will be know as the style of the image. The loss function is defined in parameter space as the difference between the content and style of the images as laid out below: . Loss = distance((style(reference_image) - style(generated_image)) + (distance(content(original_image) - content(generated_image)) . As a workflow exercise I attempted to render the image of an F1 car in the style of Picasso! I used images of recent Racing Point and Renault F1 cars. . Setup . The notebook, models and data are contained here . I trained it on Google Colab using both Tensorflow and Pytorch (separately ofcourse!) The trained model is implemented using a webapp. The user is asked to select an image and the analysis is displayed. . . Results . . Loss = distance((style(reference_image) - style(generated_image)) + (distance(content(original_image) - content(generated_image)) original_image = a picture of an F1 car reference_image = a style image such as Picasso . . .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/style%20transfer/2020/05/18/neuralstyletransfer.html",
            "relUrl": "/ai/ml/style%20transfer/2020/05/18/neuralstyletransfer.html",
            "date": " • May 18, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Predict F1 Car Constructor from Image",
            "content": "Overview . This is an exercise based on Formula 1 cars and is aimed at predicting which team a car belongs to by analysing an image. The image can be from any angle. This is an end-to-end example ir from gathering the data, training a model and deploying the trained model using a webapp on a local server. . Setup . The training setup, data are in this repo . This is a good example of a complete workflow . collecting data using google web search (local) | cleaning the data (removing duplicate images and other rubbish) (local) | porting data to Google Colab - When i was doing this I did not have my Github a/c organised properly Otherwise I would have done git init and ported the local repo to github From Google Colab it is easy to clone the Github repo | Once the data is in repo start a new jupyter notebook and select a GPU for training. | Download the trained model to the local dir | Modify the webapp to point to the trained model and do the predictions | . Note: I only used about 75 images per class (10 classes) to cut down on the training time. . I used Fastai’s recommended Starlette web api and with a bit of playing around with the css and html file got something suitable. . This is only meant for demo purposes and to motivate me to carry on with the ret of the course. . Results . . . Suggested Improvements: . Add more training data | Clean training data | Overfit and then play around with hyperparameters | Give webapp better UI | . . .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/image%20classification/2020/05/18/f1carspredictor.html",
            "relUrl": "/ai/ml/image%20classification/2020/05/18/f1carspredictor.html",
            "date": " • May 18, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Stochastic Gradient Descent",
            "content": "Overview . ML is basically this - you make a prediction, see how far that prediction is from the truth, fine-tune your prediction and keep going until the difference between between the prediction and the actual reaches an acceptable level. If we imagine the loss function plotted as a U curve then we are trying to reach the bottom of the curve as quickly as possible ie minimise the loss. From our starting point, a guess, we then take a small step in the direction of lower loss. The small step is known as the learning rate and the direction of lower loss is worked out from the slope of the curve. . Minimising the loss . Below is a regression example. The pink dots are the initial set of data. To this we have to find a general approximation that will satisfy any new points. Our initial guess is the black line. . We then go through the fllowing sequence: Loss = New Value - Actual Value Find slope of the loss function Move down the loss curve by a small amount (the Learning Rate) Find the new loss . . Pytorch implementation . y is the actual y_hat is the prediction (based on a set of weights a) loss is the mean squared error between y_hat and y def mse(y_hat,y): return((y_hat-y)**2).mean() def update(): y_hat = x@a loss=mse(y_hat,y) loss.backward() with torch.no_grad(): a.sub_(lr * a.grad) a.grad.zero_() . References . Fast.ai | Machine Learning Mastery | D :bowtie: — .",
            "url": "https://onpointai.github.io/onpointai/ai/ml%20gradient%20descent/2020/05/17/sgd.html",
            "relUrl": "/ai/ml%20gradient%20descent/2020/05/17/sgd.html",
            "date": " • May 17, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "An Overview of AI/ML",
            "content": "The contents of this page should be treated as a set of flashcards with each giving some insight into the definition, structure and scope of AI tools in everyday use. . Note: Reinforcement Learning (think Apha Go from Deep Dream/Google) has no practical applications at the moment for me and will therefore not be discussed. . . . . . . . . . . . . It has been shown that the winner is not necessarily the smartest or the one with the best computer. Instead it is the person who can develop their ideas quickly. . . .",
            "url": "https://onpointai.github.io/onpointai/ai/ml/visuals/2020/05/15/overview.html",
            "relUrl": "/ai/ml/visuals/2020/05/15/overview.html",
            "date": " • May 15, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Reference Material",
            "content": "This is a list of links to articles, blogposts, videos, tutorials that I have found very useful on my ML journey. It serves as a useful reference point and is ever-growing. . Setup . Tensorflow or Pytorch? . Google Colab . I love articles, tutorials, videos by the following: . Adrian Rosebrock Pyimagesearch. Andrej Karpathy blog. Chris Olah blog. Andrew Ng Coursera. Francois Chollet Book. Jeremey Howard Fast.ai. Rachel Thomas Fast.ai. Chris Albon Great flashcards - Buy them!. Aakash Nain Jovian. Hannah Fry [Makes number fun!]http://www.hannahfry.co.uk(). Jason Brownlee Tutorials. . Powerful Quotes . F Chollet: “You don’t need to know everything. You don’t really need a formal background in this or that – though it helps, you don’t even need a PhD. You do, however, need to be constantly learning, be curious, read books. Don’t be “too busy” to learn, or otherwise proud of your ignorance.” “Honestly, the question is not, and has never been, “ can ML replace radiologists/etc” (which won’t happen in the foreseeable future). The question is, how can radiology/etc utilise ML to improve outcomes, decrease the cost of car, and broaden accessibility.” . | Geoff Hinton: “Read enough sp you start developing intuitions and then trust your intuitions and go for it!.” . | Andrew Ng: “Deep Learning is a superpower. With it you can make a computer see, synthesize novel art, translate languages, render a medical diagnosis, or build pieces of a car that can drive itself. If that isn’t a superpower, I don’t know what is.” . | . . . Ignore stuff below. . . You can include alert boxes …and… . . You can include info boxes Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Tables . | Tensorflow | Pytorch | |-|-| | Keras | Fastai | — .",
            "url": "https://onpointai.github.io/onpointai/markdown/2020/05/14/referencematerial.html",
            "relUrl": "/markdown/2020/05/14/referencematerial.html",
            "date": " • May 14, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://onpointai.github.io/onpointai/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://onpointai.github.io/onpointai/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Dexter D’Silva . I am an Aeronautical Engineer having worked at: Airbus Wing Shape Team (High Speed Aerodynamics). EADS Innovation Works. Racing Point Formula 1 team (Aero Design processes). . I was always interested in writing macros, developing software on a one-off basis and saw myself writing advanced macros for CATIA automation initially in the field of wing shaping. . In 2017 I came across this video by Andrew Ng and since then I have been hooked on AI/Machine Learning and want to learn as much as I can about it with a view to applying my own spin on implementing flavours of it in the work that I do. . I am pretty good at connecting the dots of technology in the field of aero design where I work with CAD and CFD. I am always working on tutorials to build intuition about AI and explore it’s feasibility in my daily work, where my domain knowledge will help me to enhance my value to my customer/employer. . The other area of interest is exploring the use of AI to analyse the huge amount of data generated by CFD, wind tunnel testing and on-track or flight testing and building tools to gain easy access to knowledge, to decipher, disseminate and democratise within the organisation. . Here is my CV . Online courses with certificates . Coursera/Stanford | Deep Learning - AI for Everyone | Zero to Deep Learning | Deep Learning in Python | Supervised Learning - Scikit Learn | NLP - Python | Statistical Thinking in Python | Unsupervised Learning with Python | Intermediate Python for Data Science | Introduction Python for Data Science | Pandas Foundation | . Other Online Courses . Fast.ai | . Inspired by Fast.ai . . :cowboy_hat_face: . . .",
          "url": "https://onpointai.github.io/onpointai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://onpointai.github.io/onpointai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}