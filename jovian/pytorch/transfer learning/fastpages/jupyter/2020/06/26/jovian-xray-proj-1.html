<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Using Deep Learning to detect Pneumonia in X-ray images | onpointai</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Using Deep Learning to detect Pneumonia in X-ray images" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Using Pytorch to develop a Deep Learning framework to predict pneumonia in X-ray images" />
<meta property="og:description" content="Using Pytorch to develop a Deep Learning framework to predict pneumonia in X-ray images" />
<link rel="canonical" href="https://onpointai.github.io/onpointai/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html" />
<meta property="og:url" content="https://onpointai.github.io/onpointai/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html" />
<meta property="og:site_name" content="onpointai" />
<meta property="og:image" content="https://onpointai.github.io/onpointai/images/xray-pneumonia-1.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-26T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Using Pytorch to develop a Deep Learning framework to predict pneumonia in X-ray images","@type":"BlogPosting","headline":"Using Deep Learning to detect Pneumonia in X-ray images","dateModified":"2020-06-26T00:00:00-05:00","datePublished":"2020-06-26T00:00:00-05:00","image":"https://onpointai.github.io/onpointai/images/xray-pneumonia-1.png","url":"https://onpointai.github.io/onpointai/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://onpointai.github.io/onpointai/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/onpointai/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://onpointai.github.io/onpointai/feed.xml" title="onpointai" /><link rel="shortcut icon" type="image/x-icon" href="/onpointai/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Using Deep Learning to detect Pneumonia in X-ray images | onpointai</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Using Deep Learning to detect Pneumonia in X-ray images" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Using Pytorch to develop a Deep Learning framework to predict pneumonia in X-ray images" />
<meta property="og:description" content="Using Pytorch to develop a Deep Learning framework to predict pneumonia in X-ray images" />
<link rel="canonical" href="https://onpointai.github.io/onpointai/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html" />
<meta property="og:url" content="https://onpointai.github.io/onpointai/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html" />
<meta property="og:site_name" content="onpointai" />
<meta property="og:image" content="https://onpointai.github.io/onpointai/images/xray-pneumonia-1.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-26T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Using Pytorch to develop a Deep Learning framework to predict pneumonia in X-ray images","@type":"BlogPosting","headline":"Using Deep Learning to detect Pneumonia in X-ray images","dateModified":"2020-06-26T00:00:00-05:00","datePublished":"2020-06-26T00:00:00-05:00","image":"https://onpointai.github.io/onpointai/images/xray-pneumonia-1.png","url":"https://onpointai.github.io/onpointai/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://onpointai.github.io/onpointai/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://onpointai.github.io/onpointai/feed.xml" title="onpointai" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/onpointai/">onpointai</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/onpointai/about/">About Me</a><a class="page-link" href="/onpointai/search/">Search</a><a class="page-link" href="/onpointai/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Using Deep Learning to detect Pneumonia in X-ray images</h1><p class="page-description">Using Pytorch to develop a Deep Learning framework to predict pneumonia in X-ray images</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-26T00:00:00-05:00" itemprop="datePublished">
        Jun 26, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/onpointai/categories/#jovian">jovian</a>
        &nbsp;
      
        <a class="category-tags-link" href="/onpointai/categories/#pytorch">pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/onpointai/categories/#transfer learning">transfer learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/onpointai/categories/#fastpages">fastpages</a>
        &nbsp;
      
        <a class="category-tags-link" href="/onpointai/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#about">About</a></li>
<li class="toc-entry toc-h1"><a href="#import-libraries">Import libraries</a></li>
<li class="toc-entry toc-h1"><a href="#colab-setup-and-getting-data-from-kaggle">Colab setup and getting data from Kaggle</a></li>
<li class="toc-entry toc-h1"><a href="#image-transforms">Image transforms</a></li>
<li class="toc-entry toc-h1"><a href="#data-exploration">Data exploration</a>
<ul>
<li class="toc-entry toc-h2"><a href="#dataloaders">Dataloaders</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#visualisation">Visualisation</a></li>
<li class="toc-entry toc-h1"><a href="#transfer-learning-model">Transfer Learning model</a>
<ul>
<li class="toc-entry toc-h2"><a href="#resnet34-model-with-our-custom-classifier">Resnet34 model with our custom classifier</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#replacing-the-pretrained-model-classifier-with-our-classifier">Replacing the pretrained model classifier with our classifier</a></li>
<li class="toc-entry toc-h1"><a href="#512--256--256---256-2-2">(512 * 256 + 256) + ( 256 *2 +2)</a></li>
<li class="toc-entry toc-h1"><a href="#setup-the-training-and-validation-model">Setup the Training (and Validation) model</a>
<ul>
<li class="toc-entry toc-h3"><a href="#nllloss-because-our-output-is-logsoftmax">NLLLoss because our output is LogSoftmax</a></li>
<li class="toc-entry toc-h3"><a href="#decay-lr-by-a-factor-of-01-every-5-epochs">Decay LR by a factor of 0.1 every 5 epochs</a></li>
<li class="toc-entry toc-h3"><a href="#visualise-the-trainingvalidation-images">Visualise the Training/Validation images</a></li>
<li class="toc-entry toc-h3"><a href="#visualise-the-predictions">Visualise the predictions</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#lessons-learned">Lessons Learned</a></li>
</ul><h1 id="about">
<a class="anchor" href="#about" aria-hidden="true"><span class="octicon octicon-link"></span></a>About</h1>

<p>This blog is towards the <a href="https://jovian.ml/forum/t/assignment-5-course-project/1563">Course Project</a> for the [Pytorch Zero to GANS] free online course(https://jovian.ml/forum/c/pytorch-zero-to-gans/18) run by <a href="https://www.jovian.ml">JOVIAN.ML</a>.</p>

<p>The course <a href="https://jovian.ml/forum/t/assignment-4-in-class-data-science-competition/1564/2">competition</a> was based on analysing protein cells with muti-label classification.</p>

<p>Therefore, to extend my understanding of dealing with medical imaging I decided to use the <a href="https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia">X-Ray image database</a> in Kaggle.</p>

<p>Seeing as I ran out of GPU hours on Kaggle because of the competition (restricted to 30hrs/week at the time of writing June 2020) I opted to use Google Colab.</p>

<p>This blog is in the form of a Jupyter notebook and inspired by <a href="https://github.com/viritaromero/Detecting-Pneumonia-in-Chest-X-Rays/blob/master/Detecting_Pneumonia.ipynb">link</a>.</p>

<p>The blog talks about getting the dataset in Google Colab, explore the dataset, develop the training model, metrics and then does some preliminary training to get a model which is then used to make a few predictions. 
I will then talk about some of the lessons learned.</p>

<blockquote>
  <p>Warning! The purpose of this blog is to outline the steps taken in a typical Machine Learning project and should be treated as such.</p>
</blockquote>

<p>**Link to non-sanitised notebook on Jovian.ML here **</p>

<h1 id="import-libraries">
<a class="anchor" href="#import-libraries" aria-hidden="true"><span class="octicon octicon-link"></span></a>Import libraries</h1>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import os
import torch
import pandas as pd
import time
import copy
import PIL
import numpy as np
from torch.utils.data import Dataset, random_split, DataLoader
from PIL import Image
import torchvision
from torchvision import datasets
import torchvision.models as models
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import torchvision.transforms as T
from sklearn.metrics import f1_score
import torch.nn.functional as F
import torch.nn as nn
from torch.optim import lr_scheduler
from collections import OrderedDict
from torchvision.utils import make_grid
from torch.autograd import Variable
import seaborn as sns
import csv
%matplotlib inline```
</code></pre></div></div>
<h1 id="colab-setup-and-getting-data-from-kaggle">
<a class="anchor" href="#colab-setup-and-getting-data-from-kaggle" aria-hidden="true"><span class="octicon octicon-link"></span></a>Colab setup and getting data from Kaggle</h1>

<p>I used Google Colab with GPU processing for this project because I had exhausted my Kaggle hours (30hrs/wk) working on the competition :( The challenge here was signing into Colab, setting up the working directoty and then linking to Kaggle and copying the data over. The size of the dataset was about 1.3Gb which wasn’t too much of a bother as Google gives each Gmail account 15Gb for free!</p>

<blockquote>
  <p>Tip: I used the monokai settings in Colab which gave excellent font contrast and colours for editing.
<img src="/onpointai//images/xray-colab-monokai.png" alt="monokai" title="Colab Monokai Setting"></p>
</blockquote>

<hr>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from google.colab import drive
drive.mount('/content/drive', force_remount=True)
</code></pre></div></div>

<p>The default directory that is linked to the Google’s gdrive ( the one connected to the gmail address) is /content/drive/My Drive/</p>

<p>I create a project directory jovian-xray and use this as the new root directory.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root_dir = '/content/drive/My Drive/jovian-xray'
os.chdir(root_dir)
!pwd
os.mkdir('kaggle')
</code></pre></div></div>
<p>Install Kaggle in your current Colab session.
Log into Kaggle, point to the dataset and copy the API key. This downloads a kaggle.json file.
Upload this kaggle.json to Colab.</p>

<p>!pip install -q kaggle
from google.colab import files</p>

<p>Select the kaggle.json file. This will be uploaded to your current working directory which is the root_dir as specified above.
Create a ./kaggle directory  in the home directory
Copy the kaggle.json from the current directory to this new directory.
Change permissions so that it can be executed by user and group.</p>

<p>upload = files.upload()
!mkdir ~/.kaggle
!ls
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>proj_dir = os.path.join(root_dir, 'kaggle', 'chest_xray')
os.chdir(proj_dir)
!pwd
</code></pre></div></div>
<p>In the <a href="https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia">Kaggle data directory</a> page 
select New Notebook &gt; Three vertical dots, Copy API Command</p>

<p>#API key
!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia</p>

<p>!unzip chest-xray-pneumonia</p>

<p>os.listdir(proj_dir)</p>

<p>The dataset is structured into training, val and test folders, each with sub-folders of NORMAL and PNEUMONIA images.</p>

<h1 id="image-transforms">
<a class="anchor" href="#image-transforms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Image transforms</h1>

<p>We will now prepare the data for reading into Pytorch as numpy arrays using DataLoaders.</p>

<p>Havig data augmentation is a good way to get extra training data for free. However, care must be taken to ensure that the transforms requested are likely to appear in the inference (or test set).</p>

<p>The images (RGB) are normalized using the mean [0.485,0.456,0.406] and standard deviation [0.229,0.224,0.225] of that used for the Imagenet data in the Resnet model, so that the new input images have the same distribution and mean as that used in the Resnet model.</p>

<p>I have set up two transforms dictionaries, one with and one without so it would be easy to plot images and compare.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])

data_transforms = {'train' : T.Compose([
    T.Resize(224),
    T.CenterCrop(224),
    T.RandomRotation(20),
    T.RandomHorizontalFlip(),
    T.ToTensor(),
    T.Normalize(*imagenet_stats, inplace=True)
]),
'test' : T.Compose([
    T.Resize(224),
    T.CenterCrop(224),
    T.ToTensor(),
    T.Normalize(*imagenet_stats, inplace=True)
]),
'val' : T.Compose([
    T.Resize(224),
    T.CenterCrop(224),
    T.ToTensor(),
    T.Normalize(*imagenet_stats, inplace=True)
])}

data_no_transforms = {'train' : T.Compose([ T.ToTensor() ]),
'test' : T.Compose([T.ToTensor() ]),
'val' : T.Compose([T.ToTensor() ])}
</code></pre></div></div>
<h1 id="data-exploration">
<a class="anchor" href="#data-exploration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data exploration</h1>
<h2 id="dataloaders">
<a class="anchor" href="#dataloaders" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataloaders</h2>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>image_datasets = {x: datasets.ImageFolder(os.path.join(proj_dir, x),
                                          data_transforms[x]) for x in ['train', 'val','test']}

# Using the image datasets and the transforms, define the dataloaders
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=2) for x in ['train', 'val','test']}

dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}


class_names= image_datasets['train'].classes

print(class_names)
</code></pre></div></div>
<p>op: [‘NORMAL’, ‘PNEUMONIA’]</p>

<p>Processed Images
print(image_datasets[‘train’][0][0].shape)
print(image_datasets[‘train’])</p>

<p>op:
op: torch.Size([3, 224, 224])<br>
Dataset ImageFolder<br>
    Number of datapoints: 5216<br>
    Root location: /content/drive/My Drive/jovian-xray/kaggle/chest_xray/train<br>
    StandardTransform<br>
Transform: Compose(<br>
               Resize(size=224, interpolation=PIL.Image.BILINEAR)<br>
               CenterCrop(size=(224, 224))<br>
               RandomRotation(degrees=(-20, 20), resample=False, expand=False)<br>
               RandomHorizontalFlip(p=0.5)<br>
               ToTensor()<br>
               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])<br>
           )</p>

<h1 id="visualisation">
<a class="anchor" href="#visualisation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualisation</h1>
<p>The image data is converted in to Numpy arrays and then treated with the mean and std so that we can view the images as seen by the model.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def imshow(img, title=None):
    img = img.numpy().transpose((1, 2, 0))
    img = np.clip(img, 0, 1)
    plt.imshow(img)
    if title is not None:
        plt.title(title)

def raw_imshow(img, title=None):
    img = img.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    img = std * img + mean
    img = np.clip(img, 0, 1)
    plt.imshow(img)
    if title is not None:
        plt.title(title)

# Get a batch of training data
images, classes = next(iter(dataloaders['train']))
# Make a grid from batch
out = torchvision.utils.make_grid(images)
plt.figure(figsize=(8, 8))
raw_imshow(out, title=[class_names[x] for x in classes])
</code></pre></div></div>
<p><img src="https://github.com/onpointai/onpointai/blob/master/images/xray-raw-images.png" alt="xray-raw-images"></p>

<h1 id="transfer-learning-model">
<a class="anchor" href="#transfer-learning-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transfer Learning model</h1>

<h2 id="resnet34-model-with-our-custom-classifier">
<a class="anchor" href="#resnet34-model-with-our-custom-classifier" aria-hidden="true"><span class="octicon octicon-link"></span></a>Resnet34 model with our custom classifier</h2>

<p>The method of transfer learning is widely used to take advantage of the clever and hardworking chaps who have spent time to train a model on million+ images and save the trained model architecture and weights.</p>

<p>The Resnet34 model has been trained on the Imagenet database which has 1000 classes <a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a">from trombones to toilet tissue.</a></p>

<p>Resnet34 has a top-most fully connected layer to predict 1000 classes. In our case we need only two so we will remove the last fc layer and add our own.</p>

<p>Have a look <a href="https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8">here</a> for a good explanation of Resnet architectures. Briefly, the after each set of convolutions the input is added to the output. This helps to maintain the reslotion of the input, ie do not lose any features of the input model.</p>

<p>In a deep neural network the early layers capture generic features such as edges, texture  and colour while the latter layers capture more specific features such as cats ears, eyes, elephant trunks and so on.</p>

<p>So our process is take the trained resnet architecture and weights, remove the head ie the last layers that are used to predict the 1000 classes and add our own tailored to the number of classes we want to predict, which in our case is two.</p>

<p>We will do a first pass of training where the weights of the resnet model are locked ie, ie we do not want to overwrite or lose those values which will mean more GPU expense for us. Then we will unfreeze the weights and run the entire model at our prefereed laerning rate. Note, idelaly we would like to unfreeze only specific layer, say layer 1 and layer 4, which I will cover in a separate blogpost.</p>

<p>Build and train your network</p>
<ul>
  <li>
    <ol>
      <li>Load resnet-34 pre-trained network
model = models.resnet34(pretrained=True)</li>
    </ol>
  </li>
</ul>

<p>op:
The tailed output of the model summary gives:</p>

<p>)
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
In the Resnet34 architecture the final fully connected layer has in_features and 1000 out_features for the 1000 classes. But we need only two output classes.</p>

<p>So we add two linear layers to go from 512 RELU 256 and then 256 LOGSOFTMAX to 2 classes</p>

<p>Use a LogSoftmax for the final classification activation.</p>

<p>from collections import OrderedDict
classifier = nn.Sequential(OrderedDict([
                          (‘fc1’, nn.Linear(2048, 1024)),
                          (‘relu’, nn.ReLU()),
                          (‘fc2’, nn.Linear(1024, 2)),
                          (‘output’, nn.LogSoftmax(dim=1))
                          ]))</p>

<h1 id="replacing-the-pretrained-model-classifier-with-our-classifier">
<a class="anchor" href="#replacing-the-pretrained-model-classifier-with-our-classifier" aria-hidden="true"><span class="octicon octicon-link"></span></a>Replacing the pretrained model classifier with our classifier</h1>
<p>model.fc = classifier</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def freeze(model):
  # To freeze the residual layers
  for param in model.parameters():
    param.requires_grad = False
  for param in model.fc.parameters():
    param.requires_grad = True
    
def unfreeze(self):
  # Unfreeze all layers
  for param in model.parameters():
    param.requires_grad = True

</code></pre></div></div>
<p>freeze(model)</p>

<h1 id="512--256--256---256-2-2">
<a class="anchor" href="#512--256--256---256-2-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>(512 * 256 + 256) + ( 256 *2 +2)</h1>
<p>cp = count_parameters(model)
print(f’{cp} trainable parameters in frozen model  ‘)</p>

<p>op: 131842 trainable parameters in frozen model</p>

<p>Check: (512 * 256 + 256 bias) + (256 * 2 + 2 bias)</p>

<h1 id="setup-the-training-and-validation-model">
<a class="anchor" href="#setup-the-training-and-validation-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setup the Training (and Validation) model</h1>

<p>The dataset has training, val and tests which makes our lives a little bot easier ie we don’t have to do any data splitting and can set up specific transforms for each.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training the model
def train_model(model, criterion, optimizer, scheduler, num_epochs=10):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(1, num_epochs+1):
        print('Epoch {}/{}'.format(epoch, num_epochs))
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                scheduler.step()
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs, labels = inputs.to(device), labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    _, preds = torch.max(outputs, 1)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))

            # deep copy the model
            if phase == 'val' and epoch_acc &gt; best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best valid accuracy: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model
    ```
## Check if GPU is available
</code></pre></div></div>
<p>nThreads = 4
batch_size = 32
use_gpu = torch.cuda.is_available()
train_on_gpu = torch.cuda.is_available()</p>

<p>if not train_on_gpu:
    print(‘CUDA is not available.  Training on CPU …’)
else:
    print(‘CUDA is available!  Training on GPU …’)</p>

<p>device = torch.device(“cuda:0” if torch.cuda.is_available() else “cpu”)
print(device)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Run the Training (and Validation) model

Use the Adam optimizer which is the preferred optimizer because it is adaptive and adds a momentum element to the gradient stepping.

#Train a model with a pre-trained network
</code></pre></div></div>
<p>num_epochs = 10
if use_gpu:
    print (“Using GPU: “+ str(use_gpu))
    model = model.cuda()</p>

<h3 id="nllloss-because-our-output-is-logsoftmax">
<a class="anchor" href="#nllloss-because-our-output-is-logsoftmax" aria-hidden="true"><span class="octicon octicon-link"></span></a>NLLLoss because our output is LogSoftmax</h3>
<p>criterion = nn.NLLLoss()</p>

<p>#Adam optimizer with a learning rate
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.0001)
#optimizer = optim.SGD(model.fc.parameters(), lr = .1, momentum=0.9)</p>
<h3 id="decay-lr-by-a-factor-of-01-every-5-epochs">
<a class="anchor" href="#decay-lr-by-a-factor-of-01-every-5-epochs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Decay LR by a factor of 0.1 every 5 epochs</h3>
<p>exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#### Train the frozen model where the bottom layers only are frozen

</code></pre></div></div>
<p>model_fit = train_model(model, criterion, optimizer(lr=0.001), exp_lr_scheduler, num_epochs=10)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
#### Unfreeze the model and train some more
</code></pre></div></div>
<p>unfreeze(model)</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Testing

Do validation on the test set
</code></pre></div></div>
<p>def test(model, dataloaders, device):
  model.eval()
  accuracy = 0</p>

<p>model.to(device)</p>

<p>for images, labels in dataloaders[‘test’]:
    images = Variable(images)
    labels = Variable(labels)
    images, labels = images.to(device), labels.to(device)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output = model.forward(images)
ps = torch.exp(output)
equality = (labels.data == ps.max(1)[1])
accuracy += equality.type_as(torch.FloatTensor()).mean()
  
print("Testing Accuracy: {:.3f}".format(accuracy/len(dataloaders['test']))) ``` test(model, dataloaders, device)
</code></pre></div></div>

<p>Save the checkpoint</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model.class_to_idx = dataloaders['train'].dataset.class_to_idx
model.epochs = num_epochs
checkpoint = {'input_size': [2, 224, 224],
                 'batch_size': dataloaders['train'].batch_size,
                  'output_size':2,
                  'state_dict': model.state_dict(),
                  'data_transforms': data_transforms,
                  'optimizer_dict':optimizer.state_dict(),
                  'class_to_idx': model.class_to_idx,
                  'epoch': model.epochs}
save_fname= os.path.join(proj_dir, '90_checkpoint.pth')
torch.save(checkpoint,  save_fname)
</code></pre></div></div>

<h3 id="visualise-the-trainingvalidation-images">
<a class="anchor" href="#visualise-the-trainingvalidation-images" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualise the Training/Validation images</h3>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def visualize_model(model, num_images=6):
    was_training = model.training
    model.eval()
    images_so_far = 0
    fig = plt.figure()

    with torch.no_grad():
        for i, (inputs, labels) in enumerate(dataloaders['test']):
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            for j in range(inputs.size()[0]):
                images_so_far += 1
                ax = plt.subplot(num_images//2, 2, images_so_far)
                ax.axis('off')
                ax.set_title('predicted: {}'.format(class_names[preds[j]]))
                imshow(inputs.cpu().data[j])

                if images_so_far == num_images:
                    model.train(mode=was_training)
                    return
        model.train(mode=was_training)

visualize_model(model_ft)
</code></pre></div></div>

<p><img src="/onpointai//imaages/xray-visualise-model.png" alt=""></p>

<h3 id="visualise-the-predictions">
<a class="anchor" href="#visualise-the-predictions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualise the predictions</h3>

<p>print (predict(‘chest_xray/test/NORMAL/NORMAL2-IM-0348-0001.jpeg’, loaded_model))</p>

<blockquote>
  <p>(array([0.96704894, 0.03295105], dtype=float32), [‘NORMAL’, ‘PNEUMONIA’])</p>
</blockquote>

<p>The NORMAL probablility is much larger than the one for pneumonia.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>img = os.path.join(proj_dir, 'test/NORMAL/NORMAL2-IM-0347-0001.jpeg')
p, c = predict(img, loaded_model)
view_classify(img, p, c, class_names)
</code></pre></div></div>
<p><img src="/onpointai//images/xray-predict-1.png" alt=""></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>img = os.path.join(proj_dir, 'test/PNEUMONIA/person85_bacteria_421.jpeg')
p, c = predict(img, loaded_model)
view_classify(img, p, c, class_names)
</code></pre></div></div>
<p><img src="/onpointai//images/xray-predict-2.png" alt=""></p>

<h1 id="lessons-learned">
<a class="anchor" href="#lessons-learned" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lessons Learned</h1>

<ol>
  <li>There are five methods to reduce model overfitting.  Overfitting results when the model fits very well to the training data (low error) but not very well to the validation data (high error).
These are:
    <blockquote>
      <p>Get more data<br>
Data augmentation<br>
Generalizable architectures<br>
Regularisation<br>
Reduce architecture complexity</p>
    </blockquote>
  </li>
  <li>
    <p>Undertaking an online course like the <a href="(https://jovian.ml/forum/c/pytorch-zero-to-gans/18)">Jovian Zero to Gans</a> has been an excellent opportunity to immerse myself in Machine Learning. Taking part in the competition (which is ongoing) and writing this blog on the X-ray dataset has helped me to better understand important concepts such as Dataloaders, learning rate, batch size, optimizers and loss functions.</p>
  </li>
  <li>Thank you to Aakash the course instructor and the rest pf the Jovian team for the efforts in helping us to better understand such an exciting paradigm.</li>
</ol>

<hr>

<p>Hope you enjoyed it</p>

<hr>

<p><img src="/onpointai//images/onpointai_logo.gif" alt=""></p>


  </div><a class="u-url" href="/onpointai/jovian/pytorch/transfer%20learning/fastpages/jupyter/2020/06/26/jovian-xray-proj-1.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/onpointai/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/onpointai/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/onpointai/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My AI/ML Journey</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/onpointai" title="onpointai"><svg class="svg-icon grey"><use xlink:href="/onpointai/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/ddsilva333" title="ddsilva333"><svg class="svg-icon grey"><use xlink:href="/onpointai/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
